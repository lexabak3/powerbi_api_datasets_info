{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests as r \n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "tenant_id = config['tenant_id']\n",
    "\n",
    "\n",
    "def get_access_token(tenant_id, client_id, client_secret):\n",
    "    url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "    data = {\n",
    "        'client_id': client_id,\n",
    "        'scope': 'https://graph.microsoft.com/.default',\n",
    "        'client_secret': client_secret,\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    response = r.post(url, headers=headers, data=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get('access_token')\n",
    "\n",
    "\n",
    "list_additional_datasets_id = \\\n",
    "    [\n",
    "        ['configuredBy_test','dataset_name_test','dataset_id_test']\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = get_access_token(tenant_id, config['client_id'], config['client_secret'])\n",
    "\n",
    "header = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# Datasets\n",
    "responce_groups_datasets = r.get(f'https://api.powerbi.com/v1.0/myorg/groups/{groupId}/datasets', headers=header)\n",
    "responce_groups_datasets = json.loads(responce_groups_datasets.content)\n",
    "df = pd.json_normalize(responce_groups_datasets['value'])\n",
    "\n",
    "df_short = df.sort_values('createdDate', ascending=False)[['configuredBy','name','id']].copy()\n",
    "df_short_add = pd.DataFrame(data=list_additional_datasets_id, columns=['configuredBy','name','id'])\n",
    "df_short = pd.concat([df_short, df_short_add], ignore_index=True)\n",
    "dict_configuredBy = dict(zip(df_short.name, df_short.configuredBy))\n",
    "print('Datasets done')\n",
    "\n",
    "# Datasources\n",
    "url_datasets = 'https://api.powerbi.com/v1.0/myorg/datasets/'\n",
    "\n",
    "dict_tmp = {}\n",
    "for name, id in zip(df_short.name.to_list(), df_short.id.to_list()):\n",
    "    groups = r.get(url_datasets + id + '/datasources', headers=header)\n",
    "    time.sleep(1)\n",
    "    groups = json.loads(groups.content)\n",
    "    dict_tmp[name] = groups['value']\n",
    "\n",
    "df_result_grouped = pd.DataFrame.from_dict(data = dict_tmp.values())\n",
    "\n",
    "list_rows = []\n",
    "for x in dict_tmp:\n",
    "    for row in dict_tmp[x]:\n",
    "        list_rows.append([x, dict_configuredBy[x], row['datasourceType'], row['connectionDetails'], row['datasourceId'], row['gatewayId']])\n",
    "\n",
    "df_datasources = pd.DataFrame(data=list_rows, columns=['Dataset Name', 'configuredBy', 'datasourceType', 'connectionDetails', 'datasourceId', 'gatewayId'])\n",
    "print('Datasources done')\n",
    "\n",
    "\n",
    "# Refreshes\n",
    "dict_ref = {}\n",
    "\n",
    "for name, id in zip(df_short.name.to_list(), df_short.id.to_list()):\n",
    "    groups = r.get(url_datasets + id + '/refreshes', headers=header)\n",
    "    time.sleep(1)\n",
    "    groups = json.loads(groups.content)\n",
    "    if 'error' in groups.keys():\n",
    "        dict_ref[name] = 'Invalid dataset'\n",
    "    else:\n",
    "        if len(groups['value']) == 0:\n",
    "            dict_ref[name] = 'null'\n",
    "        else:\n",
    "            dict_ref[name] = groups['value'][0]\n",
    "\n",
    "list_columns = ['Dataset Name','startTime','status','refreshType', 'errorCode']\n",
    "\n",
    "list_rows = []\n",
    "for x, row in zip(df_short.name.to_list(), dict_ref.values()):\n",
    "    if row == 'null':\n",
    "        list_rows.append([x, pd.NA, '-', '-','-'])\n",
    "    elif row == 'Invalid dataset':\n",
    "        list_rows.append([x, pd.NA, '-', '-','Invalid dataset'])\n",
    "    elif type(row) == str:\n",
    "        list_rows.append([x, pd.NA, '-', '-','-'])\n",
    "    else:\n",
    "        if 'serviceExceptionJson' in list(row.keys()):\n",
    "            list_rows.append([x, row['startTime'], row['status'], row['refreshType'], json.loads(row['serviceExceptionJson'])['errorCode']])\n",
    "        else:\n",
    "            list_rows.append([x, row['startTime'], row['status'], row['refreshType'],'-'])\n",
    "\n",
    "\n",
    "df_result_ref = pd.DataFrame(data=list_rows, columns=list_columns)\n",
    "df_result_ref.startTime = pd.to_datetime(df_result_ref.startTime).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('Refreshes done')\n",
    "\n",
    "\n",
    "\n",
    "# RefreshSchedule\n",
    "dict_tmp_ref_sched = {}\n",
    "\n",
    "for name, id in zip(df_short.name.to_list(), df_short.id.to_list()):\n",
    "    groups = r.get(url_datasets + id + '/refreshSchedule', headers=header)\n",
    "    time.sleep(1)\n",
    "    groups = json.loads(groups.content)\n",
    "    if 'error' in list(groups.keys()):\n",
    "        dict_tmp_ref_sched[name] = 'error'\n",
    "    else: \n",
    "        dict_tmp_ref_sched[name] = groups['enabled']\n",
    "print('RefreshSchedule done')\n",
    "\n",
    "\n",
    "# Gateways\n",
    "url_getways = 'https://api.powerbi.com/v1.0/myorg/gateways/'\n",
    "list_gatewayId = df_datasources.gatewayId.drop_duplicates().to_list()\n",
    "\n",
    "list_columns = ['gatewayId','gateway_name','gatewayMachine']\n",
    "list_rows = []\n",
    "\n",
    "for id in list_gatewayId:\n",
    "    gateway = r.get(url_getways + id, headers=header)\n",
    "    time.sleep(1)\n",
    "    if gateway.reason == 'OK':\n",
    "        gateway = json.loads(gateway.content)\n",
    "        list_rows.append([id, gateway['name'], json.loads(gateway['gatewayAnnotation'])['gatewayMachine']])\n",
    "        # print(gateway['name'])\n",
    "        # print(json.loads(gateway['gatewayAnnotation'])['gatewayMachine'])\n",
    "    elif gateway.reason == 'Unauthorized':\n",
    "        list_rows.append([id,'Unauthorized', '-'])\n",
    "    else:\n",
    "        list_rows.append([id,'Personal cloud connect', '-'])\n",
    "    \n",
    "df_geteways = pd.DataFrame(data=list_rows, columns = list_columns)\n",
    "print('Gateways done')\n",
    "\n",
    "\n",
    "\n",
    "# Final Result \n",
    "df_result_ref['enabled'] = dict_tmp_ref_sched.values()\n",
    "df_final_result = pd.merge(df_datasources, df_result_ref, how='left', on='Dataset Name')\n",
    "df_final_result = pd.merge(df_final_result, df_geteways, how='left', on='gatewayId')\n",
    "list_columns_order = ['Dataset Name','configuredBy','startTime', 'status', 'enabled','gateway_name'\n",
    "                        , 'gatewayMachine', 'datasourceType','connectionDetails'\n",
    "                        ,'refreshType','datasourceId','gatewayId']\n",
    "\n",
    "df_final_result[list_columns_order].sort_values('startTime',ascending=False).to_excel('result.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20e40d8fc09a6690434ad602c7eb2d8de15d36ec466bfbfb0de97c7c540d7363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
